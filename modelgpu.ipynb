{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Import Libraries\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "from torch.autograd import Variable\n",
            "from sklearn.model_selection import train_test_split\n",
            "from torch.utils.data import DataLoader, TensorDataset, Dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "cuda\n"
               ]
            }
         ],
         "source": [
            "# Use GPU if available, else use CPU\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "print(device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "C:\\Users\\samje\\AppData\\Local\\Temp\\ipykernel_7596\\1228740871.py:1: DtypeWarning: Columns (68,69,70,71,72,73,74,75,76,77,78,79,80,81) have mixed types. Specify dtype option on import or set low_memory=False.\n",
                  "  df = pd.read_csv('dataset/training0_sort.csv')\n"
               ]
            }
         ],
         "source": [
            "# df = pd.read_csv('dataset/training0_sort.csv')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# df.info()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# display datatypes\n",
            "# df.dtypes"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# df.dtypes.value_counts()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "# run this\n",
            "# df.fillna(\"0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5,#\", inplace=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 38,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "C:\\Users\\samje\\AppData\\Local\\Temp\\ipykernel_7596\\387851574.py:1: DtypeWarning: Columns (69,70,71,72,73,74,75,76,77,78,79,80,81,82,83) have mixed types. Specify dtype option on import or set low_memory=False.\n",
                  "  df = pd.read_csv('dataset/training3_sort.csv')\n"
               ]
            }
         ],
         "source": [
            "df = pd.read_csv('dataset/training3_sort.csv')\n",
            "df.fillna(\"0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5,#\", inplace=True)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "# df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "# df.iloc[1,1:].values\n",
            "# type(df.iloc[1,:].values)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 25,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Our custom Dataset object\n",
            "class RubiksDataset(Dataset):\n",
            "    def __init__(self, df):\n",
            "        self.df = df\n",
            "        move_dict = {\"#\":0, \"U\":1, \"U'\":2, \"U2\":3, \"D\":4, \"D'\":5, \"D2\":6, \"L\":7, \"L'\":8, \"L2\":9, \"R\":10, \n",
            "                     \"R'\":11, \"R2\":12, \"F\":13, \"F'\":14, \"F2\":15, \"B\":16, \"B'\":17, \"B2\":18}\n",
            "        # list of list, where inner list is an entire sequence.\n",
            "        self.states = [] #is now a list of list of list :D\n",
            "        self.moves = [] #list of list\n",
            "\n",
            "        for i in range(len(df)):\n",
            "            statelist = []\n",
            "            movelist = []\n",
            "            statemoves = df.iloc[i,1:]\n",
            "            for line in statemoves:\n",
            "                split = line.split(\",\")\n",
            "                # state_as_int = int(split[0].replace(\" \",\"\"))/(10**54)\n",
            "                state_as_str_list = split[0].split(\" \")\n",
            "                state_as_int_list = [int(i) for i in state_as_str_list]\n",
            "                # above sets the input states to int values instead of string\n",
            "                # below sets the string move into an int value,\n",
            "                move_as_int = move_dict.get(split[1])\n",
            "\n",
            "                statelist.append(state_as_int_list)\n",
            "                movelist.append(move_as_int)\n",
            "            self.states.append(statelist)\n",
            "            self.moves.append(movelist)\n",
            "\n",
            "\n",
            "        \n",
            "    def __len__(self):\n",
            "        return len(self.moves)\n",
            "        \n",
            "    def __getitem__(self, index):\n",
            "        # inputs = torch.tensor(self.inputs[index]).float()\n",
            "        # outputs = torch.tensor(self.outputs[index]).float()\n",
            "        # print(self.states[index])\n",
            "        inputs = torch.tensor(self.states[index]) # list of list, each index is a state vector\n",
            "        outputs = torch.tensor(self.moves[index]) # list, each index is the move set of that sequence(index)\n",
            "        return inputs, outputs"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 39,
         "metadata": {},
         "outputs": [],
         "source": [
            "dataset = RubiksDataset(df)\n",
            "# dataloader = DataLoader(dataset, batch_size = 1, shuffle = False)\n",
            "dataloader = DataLoader(dataset, batch_size = 128, shuffle = True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "seqcount = 0\n",
            "for inputs, outputs in dataloader:\n",
            "    seqcount +=1\n",
            "    print(outputs)\n",
            "    # for i in outputs:\n",
            "    #     print(i)\n",
            "    print(seqcount)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "class RNN(torch.nn.Module):\n",
            "    def __init__(self):\n",
            "        super(RNN, self).__init__()\n",
            "        self.layers = torch.nn.Sequential(torch.nn.Linear(108, 256),\n",
            "                                          torch.nn.ReLU(),\n",
            "                                          torch.nn.Linear(256, 256),\n",
            "                                          torch.nn.ReLU(),\n",
            "                                          torch.nn.Linear(256, 128),\n",
            "                                          torch.nn.ReLU(),\n",
            "                                          torch.nn.Linear(128, 73))\n",
            "        \n",
            "    def forward(self, inputs, hidden):\n",
            "        # combined = torch.tensor([inputs, hidden]).to(inputs.device)\n",
            "        # print(inputs.shape)\n",
            "        # print(hidden.shape)\n",
            "        combined = torch.cat([inputs, hidden]).to(inputs.device)\n",
            "        out = self.layers(combined)\n",
            "        return out"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Sequential(\n",
                  "  (0): Linear(in_features=108, out_features=256, bias=True)\n",
                  "  (1): ReLU()\n",
                  "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
                  "  (3): ReLU()\n",
                  "  (4): Linear(in_features=256, out_features=128, bias=True)\n",
                  "  (5): ReLU()\n",
                  "  (6): Linear(in_features=128, out_features=73, bias=True)\n",
                  ")\n"
               ]
            }
         ],
         "source": [
            "model = RNN().to(device)\n",
            "print(model.layers)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [],
         "source": [
            "def train(model, dataloader, num_epochs, learning_rate, device):\n",
            "    criterion = torch.nn.CrossEntropyLoss()\n",
            "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
            "    # hidden = torch.tensor([0]).to(device)\n",
            "    # hidden= torch.zeros(1, 81, 54).to(device)\n",
            "    for epoch in range(num_epochs):\n",
            "        count = 0\n",
            "        loss = 0\n",
            "\n",
            "        \n",
            "        # optimizer.zero_grad()\n",
            "        for inputs, targets in dataloader:\n",
            "            # inputs = inputs.permute(1,0,2)\n",
            "            # need another loop inside.\n",
            "            correct = 0\n",
            "            pred = 0\n",
            "            \n",
            "            # reset hidden here\n",
            "            loss = 0\n",
            "            optimizer.zero_grad()\n",
            "            # \n",
            "            for i in range(len(inputs)):\n",
            "                hidden= torch.zeros(54).to(device)\n",
            "                for j in range(len(inputs[i])):\n",
            "                    outputs = model(inputs[i][j].to(device), hidden)\n",
            "                    out, hidden = outputs[0:19], outputs[19::]\n",
            "                    # print(out.shape)\n",
            "                    # print(hidden.shape)\n",
            "                    # print(targets[i][j].shape)\n",
            "\n",
            "                    # print((torch.argmax(out) == targets[i][j]).item())\n",
            "                    correct += (torch.argmax(out) == targets[i][j]).item()\n",
            "                    pred += 1\n",
            "                    loss += criterion(out.to(device), targets[i][j].to(device)) / len(inputs[i]) / len(inputs)\n",
            "                # print(loss)\n",
            "            batch_accuracy = correct/pred\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "            count +=1\n",
            "            print(f\"Batch number: {count}\")\n",
            "            print(f\"Batch {count} accuracy: {batch_accuracy}\")\n",
            "        \n",
            "        # loss /= len(inputs)\n",
            "        # loss.backward()\n",
            "        # optimizer.step()\n",
            "\n",
            "        #     outputs = model(inputs.to(device), hidden)\n",
            "        #     out, hidden = outputs[0], outputs[1]\n",
            "        #     loss += criterion(out.to(device), targets.to(device))\n",
            "        # loss /= len(dataloader)\n",
            "        # loss.backward()\n",
            "        # optimizer.step()\n",
            "\n",
            "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Last Batch Accuracy: {batch_accuracy*100:.2f}%\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 41,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Batch number: 1\n",
                  "Batch 1 accuracy: 0.7003012048192772\n",
                  "Batch number: 2\n",
                  "Batch 2 accuracy: 0.6601091867469879\n",
                  "Batch number: 3\n",
                  "Batch 3 accuracy: 0.686558734939759\n",
                  "Batch number: 4\n",
                  "Batch 4 accuracy: 0.6935240963855421\n",
                  "Batch number: 5\n",
                  "Batch 5 accuracy: 0.6785579819277109\n",
                  "Batch number: 6\n",
                  "Batch 6 accuracy: 0.6912650602409639\n",
                  "Batch number: 7\n",
                  "Batch 7 accuracy: 0.6926769578313253\n",
                  "Batch number: 8\n",
                  "Batch 8 accuracy: 0.6913591867469879\n",
                  "Batch number: 9\n",
                  "Batch 9 accuracy: 0.686558734939759\n",
                  "Batch number: 10\n",
                  "Batch 10 accuracy: 0.6947477409638554\n",
                  "Batch number: 11\n",
                  "Batch 11 accuracy: 0.6895707831325302\n",
                  "Batch number: 12\n",
                  "Batch 12 accuracy: 0.6980421686746988\n",
                  "Batch number: 13\n",
                  "Batch 13 accuracy: 0.6963478915662651\n",
                  "Batch number: 14\n",
                  "Batch 14 accuracy: 0.7030308734939759\n",
                  "Batch number: 15\n",
                  "Batch 15 accuracy: 0.6944653614457831\n",
                  "Batch number: 16\n",
                  "Batch 16 accuracy: 0.7008659638554217\n",
                  "Batch number: 17\n",
                  "Batch 17 accuracy: 0.697289156626506\n",
                  "Batch number: 18\n",
                  "Batch 18 accuracy: 0.7027484939759037\n",
                  "Batch number: 19\n",
                  "Batch 19 accuracy: 0.6997364457831325\n",
                  "Batch number: 20\n",
                  "Batch 20 accuracy: 0.7043486445783133\n",
                  "Batch number: 21\n",
                  "Batch 21 accuracy: 0.7008659638554217\n",
                  "Batch number: 22\n",
                  "Batch 22 accuracy: 0.7035015060240963\n",
                  "Batch number: 23\n",
                  "Batch 23 accuracy: 0.7056664156626506\n",
                  "Batch number: 24\n",
                  "Batch 24 accuracy: 0.698230421686747\n",
                  "Batch number: 25\n",
                  "Batch 25 accuracy: 0.6989834337349398\n",
                  "Batch number: 26\n",
                  "Batch 26 accuracy: 0.7055722891566265\n",
                  "Batch number: 27\n",
                  "Batch 27 accuracy: 0.7032191265060241\n",
                  "Batch number: 28\n",
                  "Batch 28 accuracy: 0.7072665662650602\n",
                  "Batch number: 29\n",
                  "Batch 29 accuracy: 0.7056664156626506\n",
                  "Batch number: 30\n",
                  "Batch 30 accuracy: 0.7041603915662651\n",
                  "Batch number: 31\n",
                  "Batch 31 accuracy: 0.7013365963855421\n",
                  "Batch number: 32\n",
                  "Batch 32 accuracy: 0.7027484939759037\n",
                  "Batch number: 33\n",
                  "Batch 33 accuracy: 0.6968185240963856\n",
                  "Batch number: 34\n",
                  "Batch 34 accuracy: 0.6983245481927711\n",
                  "Batch number: 35\n",
                  "Batch 35 accuracy: 0.7093373493975904\n",
                  "Batch number: 36\n",
                  "Batch 36 accuracy: 0.7034073795180723\n",
                  "Batch number: 37\n",
                  "Batch 37 accuracy: 0.7044427710843374\n",
                  "Batch number: 38\n",
                  "Batch 38 accuracy: 0.702183734939759\n",
                  "Batch number: 39\n",
                  "Batch 39 accuracy: 0.7017131024096386\n",
                  "Batch number: 40\n",
                  "Batch 40 accuracy: 0.7019954819277109\n",
                  "Batch number: 41\n",
                  "Batch 41 accuracy: 0.7131024096385542\n",
                  "Batch number: 42\n",
                  "Batch 42 accuracy: 0.7098079819277109\n",
                  "Batch number: 43\n",
                  "Batch 43 accuracy: 0.7048192771084337\n",
                  "Batch number: 44\n",
                  "Batch 44 accuracy: 0.7077371987951807\n",
                  "Batch number: 45\n",
                  "Batch 45 accuracy: 0.7020896084337349\n",
                  "Batch number: 46\n",
                  "Batch 46 accuracy: 0.7103727409638554\n",
                  "Batch number: 47\n",
                  "Batch 47 accuracy: 0.7067018072289156\n",
                  "Batch number: 48\n",
                  "Batch 48 accuracy: 0.705101656626506\n",
                  "Batch number: 49\n",
                  "Batch 49 accuracy: 0.7055722891566265\n",
                  "Batch number: 50\n",
                  "Batch 50 accuracy: 0.7108433734939759\n",
                  "Batch number: 51\n",
                  "Batch 51 accuracy: 0.7045368975903614\n",
                  "Batch number: 52\n",
                  "Batch 52 accuracy: 0.7013365963855421\n",
                  "Batch number: 53\n",
                  "Batch 53 accuracy: 0.7117846385542169\n",
                  "Batch number: 54\n",
                  "Batch 54 accuracy: 0.7074548192771084\n",
                  "Batch number: 55\n",
                  "Batch 55 accuracy: 0.7054781626506024\n",
                  "Batch number: 56\n",
                  "Batch 56 accuracy: 0.7121611445783133\n",
                  "Batch number: 57\n",
                  "Batch 57 accuracy: 0.7103727409638554\n",
                  "Batch number: 58\n",
                  "Batch 58 accuracy: 0.7081137048192772\n",
                  "Batch number: 59\n",
                  "Batch 59 accuracy: 0.7128200301204819\n",
                  "Batch number: 60\n",
                  "Batch 60 accuracy: 0.7065135542168675\n",
                  "Batch number: 61\n",
                  "Batch 61 accuracy: 0.7168674698795181\n",
                  "Batch number: 62\n",
                  "Batch 62 accuracy: 0.7095256024096386\n",
                  "Batch number: 63\n",
                  "Batch 63 accuracy: 0.7161144578313253\n",
                  "Batch number: 64\n",
                  "Batch 64 accuracy: 0.7065135542168675\n",
                  "Batch number: 65\n",
                  "Batch 65 accuracy: 0.7101844879518072\n",
                  "Batch number: 66\n",
                  "Batch 66 accuracy: 0.7046310240963856\n",
                  "Batch number: 67\n",
                  "Batch 67 accuracy: 0.7083019578313253\n",
                  "Batch number: 68\n",
                  "Batch 68 accuracy: 0.7109375\n",
                  "Batch number: 69\n",
                  "Batch 69 accuracy: 0.709996234939759\n",
                  "Batch number: 70\n",
                  "Batch 70 accuracy: 0.7142319277108434\n",
                  "Batch number: 71\n",
                  "Batch 71 accuracy: 0.7174322289156626\n",
                  "Batch number: 72\n",
                  "Batch 72 accuracy: 0.7105609939759037\n",
                  "Batch number: 73\n",
                  "Batch 73 accuracy: 0.7103727409638554\n",
                  "Batch number: 74\n",
                  "Batch 74 accuracy: 0.7099021084337349\n",
                  "Batch number: 75\n",
                  "Batch 75 accuracy: 0.7121611445783133\n",
                  "Batch number: 76\n",
                  "Batch 76 accuracy: 0.7075489457831325\n",
                  "Batch number: 77\n",
                  "Batch 77 accuracy: 0.7039721385542169\n",
                  "Batch number: 78\n",
                  "Batch 78 accuracy: 0.712914156626506\n",
                  "Batch number: 79\n",
                  "Batch 79 accuracy: 0.7100903614457831\n",
                  "Epoch 1/1, Loss: 0.8276, Last Batch Accuracy: 71.01%\n"
               ]
            }
         ],
         "source": [
            "# Train the model\n",
            "# model = RNN().to(device)\n",
            "train(model, dataloader, num_epochs = 1, learning_rate = 0.0005, device = device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 42,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Save the model use new names each time tyvm\n",
            "torch.save(model.state_dict(), 'weights/rnn_4layer_256_14.pth')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "<All keys matched successfully>"
                  ]
               },
               "execution_count": 15,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Load the model\n",
            "model = RNN().to(device)\n",
            "model.load_state_dict(torch.load('weights/rnn_4layer_256_6.pth'))\n",
            "# model.load_state_dict(torch.load('weights/rnn_4layer_256_6.pth', map_location=torch.device('cpu')))\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "C:\\Users\\samje\\AppData\\Local\\Temp\\ipykernel_7596\\414264106.py:1: DtypeWarning: Columns (69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
                  "  test_dataset = pd.read_csv(\"dataset/training99_sort.csv\")\n"
               ]
            }
         ],
         "source": [
            "test_dataset = pd.read_csv(\"dataset/training99_sort.csv\")\n",
            "test_dataset.fillna(\"0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5,#\", inplace=True)\n",
            "test_rubiksdataset = RubiksDataset(test_dataset)\n",
            "test_dataloader = DataLoader(test_rubiksdataset, batch_size = 128)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 21,
         "metadata": {},
         "outputs": [],
         "source": [
            "def test(model, test_loader, device):\n",
            "    model.eval()\n",
            "    # test_loss = 0\n",
            "    correct = 0\n",
            "    pred = 0\n",
            "    count = 0\n",
            "    sumofaccuracy = 0\n",
            "    # criterion = torch.nn.CrossEntropyLoss()\n",
            "    with torch.no_grad():\n",
            "        for data, target in test_loader:\n",
            "            for i in range(len(data)):\n",
            "                hidden= torch.zeros(54).to(device)\n",
            "                for j in range(len(data[i])):\n",
            "                    output = model(data[i][j].to(device), hidden)\n",
            "                    out, hidden = output[0:19], output[19::]\n",
            "                    # test_loss += criterion(out.to(device), target[i][j].to(device)) / len(data[i]) / len(data)\n",
            "                    # pred = out.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
            "                    # correct += pred.eq(target.view_as(pred)).sum().item()\n",
            "                    # if args.dry_run:\n",
            "                    #     break\n",
            "                    correct += (torch.argmax(out) == target[i][j]).item()\n",
            "                    pred += 1\n",
            "            batch_accuracy = correct/pred\n",
            "\n",
            "    # test_loss /= len(test_loader.dataset)\n",
            "\n",
            "    # print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
            "    #     test_loss, correct, len(test_loader.dataset),\n",
            "    #     100. * correct / len(test_loader.dataset)))\n",
            "            count +=1\n",
            "            print(f\"Batch {count} accuracy: {batch_accuracy}\")\n",
            "            sumofaccuracy += batch_accuracy\n",
            "    print(f\"Total Test accuracy: {sumofaccuracy/count} , for {count} batch in total.\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 43,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Batch 1 accuracy: 0.7638578869047619\n",
                  "Batch 2 accuracy: 0.7575799851190477\n",
                  "Batch 3 accuracy: 0.7565724206349206\n",
                  "Batch 4 accuracy: 0.7555338541666666\n",
                  "Batch 5 accuracy: 0.7542410714285714\n",
                  "Batch 6 accuracy: 0.7521236359126984\n",
                  "Batch 7 accuracy: 0.7504517431972789\n",
                  "Batch 8 accuracy: 0.7488723028273809\n",
                  "Batch 9 accuracy: 0.7474371693121693\n",
                  "Batch 10 accuracy: 0.7462146577380953\n",
                  "Batch 11 accuracy: 0.745019954004329\n",
                  "Batch 12 accuracy: 0.7439778645833334\n",
                  "Batch 13 accuracy: 0.7430317078754579\n",
                  "Batch 14 accuracy: 0.7421077806122449\n",
                  "Batch 15 accuracy: 0.7412574404761905\n",
                  "Batch 16 accuracy: 0.7406877790178571\n",
                  "Batch 17 accuracy: 0.7399334733893558\n",
                  "Batch 18 accuracy: 0.7394128224206349\n",
                  "Batch 19 accuracy: 0.7385896381578947\n",
                  "Batch 20 accuracy: 0.7377464657738095\n",
                  "Batch 21 accuracy: 0.7371120323129252\n",
                  "Batch 22 accuracy: 0.7363492627164502\n",
                  "Batch 23 accuracy: 0.7357700892857143\n",
                  "Batch 24 accuracy: 0.7350725446428571\n",
                  "Batch 25 accuracy: 0.7343415178571429\n",
                  "Batch 26 accuracy: 0.7338205414377289\n",
                  "Batch 27 accuracy: 0.7332107032627866\n",
                  "Batch 28 accuracy: 0.7325314891581632\n",
                  "Batch 29 accuracy: 0.7320658866995073\n",
                  "Batch 30 accuracy: 0.7318855406746032\n",
                  "Batch 31 accuracy: 0.7314588133640553\n",
                  "Batch 32 accuracy: 0.7308814639136905\n",
                  "Batch 33 accuracy: 0.7304743867243867\n",
                  "Batch 34 accuracy: 0.730055694152661\n",
                  "Batch 35 accuracy: 0.7297167304421769\n",
                  "Batch 36 accuracy: 0.7294560185185185\n",
                  "Batch 37 accuracy: 0.7290309282496783\n",
                  "Batch 38 accuracy: 0.7287089794799498\n",
                  "Batch 39 accuracy: 0.7283796932234432\n",
                  "Batch 40 accuracy: 0.7279878162202381\n",
                  "Batch 41 accuracy: 0.7276921820557491\n",
                  "Batch 42 accuracy: 0.7272046839569161\n",
                  "Batch 43 accuracy: 0.726735534330011\n",
                  "Batch 44 accuracy: 0.7262475480248918\n",
                  "Batch 45 accuracy: 0.7258101851851851\n",
                  "Batch 46 accuracy: 0.725296810300207\n",
                  "Batch 47 accuracy: 0.7248943294072948\n",
                  "Batch 48 accuracy: 0.7244466145833334\n",
                  "Batch 49 accuracy: 0.7240342565597667\n",
                  "Batch 50 accuracy: 0.7234933035714286\n",
                  "Batch 51 accuracy: 0.7230993960084033\n",
                  "Batch 52 accuracy: 0.7226580385760073\n",
                  "Batch 53 accuracy: 0.7222930003369272\n",
                  "Batch 54 accuracy: 0.7219690393518519\n",
                  "Batch 55 accuracy: 0.721582454004329\n",
                  "Batch 56 accuracy: 0.7211465640943877\n",
                  "Batch 57 accuracy: 0.7206998616332498\n",
                  "Batch 58 accuracy: 0.7204561781609196\n",
                  "Batch 59 accuracy: 0.720012674031477\n",
                  "Batch 60 accuracy: 0.719638206845238\n",
                  "Batch 61 accuracy: 0.7193492022833724\n",
                  "Batch 62 accuracy: 0.7190545194892473\n",
                  "Batch 63 accuracy: 0.7186702806122449\n",
                  "Batch 64 accuracy: 0.7182835170200893\n",
                  "Batch 65 accuracy: 0.7179515796703296\n",
                  "Batch 66 accuracy: 0.7175071022727273\n",
                  "Batch 67 accuracy: 0.7172577403162758\n",
                  "Batch 68 accuracy: 0.7168310683648459\n",
                  "Batch 69 accuracy: 0.7165111175810904\n",
                  "Batch 70 accuracy: 0.7161192602040817\n",
                  "Batch 71 accuracy: 0.7156991427733065\n",
                  "Batch 72 accuracy: 0.7153165302579365\n",
                  "Batch 73 accuracy: 0.7148819716242661\n",
                  "Batch 74 accuracy: 0.7144868082368082\n",
                  "Batch 75 accuracy: 0.7140414186507936\n",
                  "Batch 76 accuracy: 0.7135893934053885\n",
                  "Batch 77 accuracy: 0.7130814683828076\n",
                  "Batch 78 accuracy: 0.7123695531898657\n",
                  "Batch 79 accuracy: 0.7122666666666667\n",
                  "Total Test accuracy: 0.7298182150368241 , for 79 batch in total.\n"
               ]
            }
         ],
         "source": [
            "test(model, test_dataloader, device=device)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# shawns stuff"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# split data into features\n",
            "targets_numpy = train.label.values\n",
            "features_numpy = train.loc[:,train.columns != \"label\"].values/255 # normalization\n",
            "\n",
            "# train test split. Size of train data is 80% and size of test data is 20%. \n",
            "features_train, features_test, targets_train, targets_test = train_test_split(features_numpy,\n",
            "                                                                             targets_numpy,\n",
            "                                                                             test_size = 0.2,\n",
            "                                                                             random_state = 42) \n",
            "\n",
            "# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
            "featuresTrain = torch.from_numpy(features_train)\n",
            "targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n",
            "\n",
            "# create feature and targets tensor for test set.\n",
            "featuresTest = torch.from_numpy(features_test)\n",
            "targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n",
            "\n",
            "# batch_size, epoch and iteration\n",
            "batch_size = 100\n",
            "n_iters = 10000\n",
            "num_epochs = n_iters / (len(features_train) / batch_size)\n",
            "num_epochs = int(num_epochs)\n",
            "\n",
            "# Pytorch train and test sets\n",
            "train = TensorDataset(featuresTrain,targetsTrain)\n",
            "test = TensorDataset(featuresTest,targetsTest)\n",
            "\n",
            "# data loader\n",
            "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
            "test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n",
            "\n",
            "# visualize one of the images in data set\n",
            "plt.imshow(features_numpy[10].reshape(28,28))\n",
            "plt.axis(\"off\")\n",
            "plt.title(str(targets_numpy[10]))\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "ename": "NameError",
               "evalue": "name 'features_train' is not defined",
               "output_type": "error",
               "traceback": [
                  "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                  "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     30\u001b[0m n_iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8000\u001b[39m\n\u001b[1;32m---> 31\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m n_iters \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[43mfeatures_train\u001b[49m) \u001b[38;5;241m/\u001b[39m batch_size)\n\u001b[0;32m     32\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(num_epochs)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Pytorch train and test sets\u001b[39;00m\n",
                  "\u001b[1;31mNameError\u001b[0m: name 'features_train' is not defined"
               ]
            }
         ],
         "source": [
            "# Create RNN Model\n",
            "class RNNModel(nn.Module):\n",
            "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
            "        super(RNNModel, self).__init__()\n",
            "        \n",
            "        # Number of hidden dimensions\n",
            "        self.hidden_dim = hidden_dim\n",
            "        \n",
            "        # Number of hidden layers\n",
            "        self.layer_dim = layer_dim\n",
            "        \n",
            "        # RNN\n",
            "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
            "        \n",
            "        # Readout layer\n",
            "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
            "    \n",
            "    def forward(self, x):\n",
            "        \n",
            "        # Initialize hidden state with zeros\n",
            "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
            "            \n",
            "        # One time step\n",
            "        out, hn = self.rnn(x, h0)\n",
            "        out = self.fc(out[:, -1, :]) \n",
            "        return out\n",
            "\n",
            "# batch_size, epoch and iteration\n",
            "batch_size = 100\n",
            "n_iters = 8000\n",
            "num_epochs = n_iters / (len(features_train) / batch_size)\n",
            "num_epochs = int(num_epochs)\n",
            "\n",
            "# Pytorch train and test sets\n",
            "train = TensorDataset(featuresTrain,targetsTrain)\n",
            "test = TensorDataset(featuresTest,targetsTest)\n",
            "\n",
            "# data loader\n",
            "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
            "test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n",
            "    \n",
            "# Create RNN\n",
            "input_dim = 28    # input dimension\n",
            "hidden_dim = 100  # hidden layer dimension\n",
            "layer_dim = 1     # number of hidden layers\n",
            "output_dim = 10   # output dimension\n",
            "\n",
            "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
            "\n",
            "# Cross Entropy Loss \n",
            "error = nn.CrossEntropyLoss()\n",
            "\n",
            "# SGD Optimizer\n",
            "learning_rate = 0.05\n",
            "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Training model\n",
            "seq_dim = 28  \n",
            "loss_list = []\n",
            "iteration_list = []\n",
            "accuracy_list = []\n",
            "count = 0\n",
            "for epoch in range(num_epochs):\n",
            "    for i, (images, labels) in enumerate(train_loader):\n",
            "\n",
            "        train  = Variable(images.view(-1, seq_dim, input_dim))\n",
            "        labels = Variable(labels )\n",
            "            \n",
            "        # Clear gradients\n",
            "        optimizer.zero_grad()\n",
            "        \n",
            "        # Forward propagation\n",
            "        outputs = model(train)\n",
            "        \n",
            "        # Calculate softmax and ross entropy loss\n",
            "        loss = error(outputs, labels)\n",
            "        \n",
            "        # Calculating gradients\n",
            "        loss.backward()\n",
            "        \n",
            "        # Update parameters\n",
            "        optimizer.step()\n",
            "        \n",
            "        count += 1\n",
            "        \n",
            "        if count % 250 == 0:\n",
            "            # Calculate Accuracy         \n",
            "            correct = 0\n",
            "            total = 0\n",
            "            # Iterate through test dataset\n",
            "            for images, labels in test_loader:\n",
            "                images = Variable(images.view(-1, seq_dim, input_dim))\n",
            "                \n",
            "                # Forward propagation\n",
            "                outputs = model(images)\n",
            "                \n",
            "                # Get predictions from the maximum value\n",
            "                predicted = torch.max(outputs.data, 1)[1]\n",
            "                \n",
            "                # Total number of labels\n",
            "                total += labels.size(0)\n",
            "                \n",
            "                correct += (predicted == labels).sum()\n",
            "            \n",
            "            accuracy = 100 * correct / float(total)\n",
            "            \n",
            "            # store loss and iteration\n",
            "            loss_list.append(loss.data)\n",
            "            iteration_list.append(count)\n",
            "            accuracy_list.append(accuracy)\n",
            "            if count % 500 == 0:\n",
            "                # Print Loss\n",
            "                print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data[0], accuracy))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# visualization loss \n",
            "plt.plot(iteration_list,loss_list)\n",
            "plt.xlabel(\"Number of iteration\")\n",
            "plt.ylabel(\"Loss\")\n",
            "plt.title(\"RNN: Loss vs Number of iteration\")\n",
            "plt.show()\n",
            "\n",
            "# visualization accuracy \n",
            "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
            "plt.xlabel(\"Number of iteration\")\n",
            "plt.ylabel(\"Accuracy\")\n",
            "plt.title(\"RNN: Accuracy vs Number of iteration\")\n",
            "plt.savefig('graph.png')\n",
            "plt.show()"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.11"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
