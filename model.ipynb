{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "import numpy as np\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Import Libraries\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "from torch.autograd import Variable\n",
            "from sklearn.model_selection import train_test_split\n",
            "from torch.utils.data import DataLoader, TensorDataset, Dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "cpu\n"
               ]
            }
         ],
         "source": [
            "# Use GPU if available, else use CPU\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "print(device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/var/folders/8p/93744ql90kj6j170t1qsdthm0000gn/T/ipykernel_81908/1228740871.py:1: DtypeWarning: Columns (68,69,70,71,72,73,74,75,76,77,78,79,80,81) have mixed types. Specify dtype option on import or set low_memory=False.\n",
                  "  df = pd.read_csv('dataset/training0_sort.csv')\n"
               ]
            }
         ],
         "source": [
            "df = pd.read_csv('dataset/training0_sort.csv')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "# df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "<class 'pandas.core.frame.DataFrame'>\n",
                  "RangeIndex: 10000 entries, 0 to 9999\n",
                  "Data columns (total 82 columns):\n",
                  " #   Column  Non-Null Count  Dtype \n",
                  "---  ------  --------------  ----- \n",
                  " 0   sol     10000 non-null  int64 \n",
                  " 1   0       10000 non-null  object\n",
                  " 2   1       10000 non-null  object\n",
                  " 3   2       10000 non-null  object\n",
                  " 4   3       10000 non-null  object\n",
                  " 5   4       10000 non-null  object\n",
                  " 6   5       10000 non-null  object\n",
                  " 7   6       10000 non-null  object\n",
                  " 8   7       10000 non-null  object\n",
                  " 9   8       10000 non-null  object\n",
                  " 10  9       10000 non-null  object\n",
                  " 11  10      10000 non-null  object\n",
                  " 12  11      10000 non-null  object\n",
                  " 13  12      10000 non-null  object\n",
                  " 14  13      10000 non-null  object\n",
                  " 15  14      10000 non-null  object\n",
                  " 16  15      10000 non-null  object\n",
                  " 17  16      10000 non-null  object\n",
                  " 18  17      10000 non-null  object\n",
                  " 19  18      10000 non-null  object\n",
                  " 20  19      10000 non-null  object\n",
                  " 21  20      10000 non-null  object\n",
                  " 22  21      10000 non-null  object\n",
                  " 23  22      10000 non-null  object\n",
                  " 24  23      10000 non-null  object\n",
                  " 25  24      10000 non-null  object\n",
                  " 26  25      10000 non-null  object\n",
                  " 27  26      10000 non-null  object\n",
                  " 28  27      10000 non-null  object\n",
                  " 29  28      10000 non-null  object\n",
                  " 30  29      10000 non-null  object\n",
                  " 31  30      10000 non-null  object\n",
                  " 32  31      10000 non-null  object\n",
                  " 33  32      10000 non-null  object\n",
                  " 34  33      10000 non-null  object\n",
                  " 35  34      10000 non-null  object\n",
                  " 36  35      10000 non-null  object\n",
                  " 37  36      10000 non-null  object\n",
                  " 38  37      10000 non-null  object\n",
                  " 39  38      9999 non-null   object\n",
                  " 40  39      9999 non-null   object\n",
                  " 41  40      9997 non-null   object\n",
                  " 42  41      9992 non-null   object\n",
                  " 43  42      9987 non-null   object\n",
                  " 44  43      9981 non-null   object\n",
                  " 45  44      9974 non-null   object\n",
                  " 46  45      9967 non-null   object\n",
                  " 47  46      9954 non-null   object\n",
                  " 48  47      9937 non-null   object\n",
                  " 49  48      9903 non-null   object\n",
                  " 50  49      9865 non-null   object\n",
                  " 51  50      9823 non-null   object\n",
                  " 52  51      9753 non-null   object\n",
                  " 53  52      9654 non-null   object\n",
                  " 54  53      9524 non-null   object\n",
                  " 55  54      9342 non-null   object\n",
                  " 56  55      9117 non-null   object\n",
                  " 57  56      8823 non-null   object\n",
                  " 58  57      8433 non-null   object\n",
                  " 59  58      7952 non-null   object\n",
                  " 60  59      7394 non-null   object\n",
                  " 61  60      6735 non-null   object\n",
                  " 62  61      6012 non-null   object\n",
                  " 63  62      5249 non-null   object\n",
                  " 64  63      4484 non-null   object\n",
                  " 65  64      3686 non-null   object\n",
                  " 66  65      3009 non-null   object\n",
                  " 67  66      2356 non-null   object\n",
                  " 68  67      1795 non-null   object\n",
                  " 69  68      1304 non-null   object\n",
                  " 70  69      922 non-null    object\n",
                  " 71  70      652 non-null    object\n",
                  " 72  71      459 non-null    object\n",
                  " 73  72      313 non-null    object\n",
                  " 74  73      199 non-null    object\n",
                  " 75  74      119 non-null    object\n",
                  " 76  75      79 non-null     object\n",
                  " 77  76      50 non-null     object\n",
                  " 78  77      29 non-null     object\n",
                  " 79  78      12 non-null     object\n",
                  " 80  79      5 non-null      object\n",
                  " 81  80      2 non-null      object\n",
                  "dtypes: int64(1), object(81)\n",
                  "memory usage: 6.3+ MB\n"
               ]
            }
         ],
         "source": [
            "df.info()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "sol     int64\n",
                     "0      object\n",
                     "1      object\n",
                     "2      object\n",
                     "3      object\n",
                     "        ...  \n",
                     "76     object\n",
                     "77     object\n",
                     "78     object\n",
                     "79     object\n",
                     "80     object\n",
                     "Length: 82, dtype: object"
                  ]
               },
               "execution_count": 7,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# display datatypes\n",
            "df.dtypes"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "object    81\n",
                     "int64      1\n",
                     "dtype: int64"
                  ]
               },
               "execution_count": 8,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "df.dtypes.value_counts()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [],
         "source": [
            "# run this\n",
            "df.fillna(\"0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5,#\", inplace=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "# df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Prepare Dataset\n",
            "# load data\n",
            "# df = pd.read_csv('dataset/training0_sort.csv',dtype = np.float32)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "# df.iloc[1,1:].values\n",
            "# type(df.iloc[1,:].values)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Our custom Dataset object\n",
            "class RubiksDataset(Dataset):\n",
            "    def __init__(self, df):\n",
            "        self.df = df\n",
            "        move_dict = {\"#\":0, \"U\":1, \"U'\":2, \"U2\":3, \"D\":4, \"D'\":5, \"D2\":6, \"L\":7, \"L'\":8, \"L2\":9, \"R\":10, \n",
            "                     \"R'\":11, \"R2\":12, \"F\":13, \"F'\":14, \"F2\":15, \"B\":16, \"B'\":17, \"B2\":18}\n",
            "        # list of list, where inner list is an entire sequence.\n",
            "        self.states = [] #is now a list of list of list :D\n",
            "        self.moves = [] #list of list\n",
            "\n",
            "        for i in range(len(df)):\n",
            "            statelist = []\n",
            "            movelist = []\n",
            "            statemoves = df.iloc[i,1:]\n",
            "            for line in statemoves:\n",
            "                split = line.split(\",\")\n",
            "                # state_as_int = int(split[0].replace(\" \",\"\"))/(10**54)\n",
            "                state_as_str_list = split[0].split(\" \")\n",
            "                state_as_int_list = [int(i) for i in state_as_str_list]\n",
            "                # above sets the input states to int values instead of string\n",
            "                # below sets the string move into an int value,\n",
            "                move_as_int = move_dict.get(split[1])\n",
            "\n",
            "                statelist.append(state_as_int_list)\n",
            "                movelist.append(move_as_int)\n",
            "            self.states.append(statelist)\n",
            "            self.moves.append(movelist)\n",
            "\n",
            "\n",
            "        \n",
            "    def __len__(self):\n",
            "        return len(self.moves)\n",
            "        \n",
            "    def __getitem__(self, index):\n",
            "        # inputs = torch.tensor(self.inputs[index]).float()\n",
            "        # outputs = torch.tensor(self.outputs[index]).float()\n",
            "        # print(self.states[index])\n",
            "        inputs = torch.tensor(self.states[index]) # list of list, each index is a state vector\n",
            "        outputs = torch.tensor(self.moves[index]) # list, each index is the move set of that sequence(index)\n",
            "        return inputs, outputs"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [],
         "source": [
            "dataset = RubiksDataset(df)\n",
            "# dataloader = DataLoader(dataset, batch_size = 1, shuffle = False)\n",
            "dataloader = DataLoader(dataset, batch_size = 128, shuffle = True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "seqcount = 0\n",
            "for inputs, outputs in dataloader:\n",
            "    seqcount +=1\n",
            "    print(outputs)\n",
            "    # for i in outputs:\n",
            "    #     print(i)\n",
            "    print(seqcount)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [],
         "source": [
            "class RNN(torch.nn.Module):\n",
            "    def __init__(self):\n",
            "        super(RNN, self).__init__()\n",
            "        self.layers = torch.nn.Sequential(torch.nn.Linear(108, 256),\n",
            "                                          torch.nn.ReLU(),\n",
            "                                          torch.nn.Linear(256, 128),\n",
            "                                          torch.nn.ReLU(),\n",
            "                                          torch.nn.Linear(128, 73))\n",
            "        \n",
            "    def forward(self, inputs, hidden):\n",
            "        # combined = torch.tensor([inputs, hidden]).to(inputs.device)\n",
            "        # print(inputs.shape)\n",
            "        # print(hidden.shape)\n",
            "        combined = torch.cat([inputs, hidden]).to(inputs.device)\n",
            "        out = self.layers(combined)\n",
            "        return out"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Sequential(\n",
                  "  (0): Linear(in_features=108, out_features=256, bias=True)\n",
                  "  (1): ReLU()\n",
                  "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
                  "  (3): ReLU()\n",
                  "  (4): Linear(in_features=128, out_features=73, bias=True)\n",
                  ")\n"
               ]
            }
         ],
         "source": [
            "model = RNN().to(device)\n",
            "print(model.layers)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 35,
         "metadata": {},
         "outputs": [],
         "source": [
            "def train(model, dataloader, num_epochs, learning_rate, device):\n",
            "    criterion = torch.nn.CrossEntropyLoss()\n",
            "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
            "    # hidden = torch.tensor([0]).to(device)\n",
            "    # hidden= torch.zeros(1, 81, 54).to(device)\n",
            "    for epoch in range(num_epochs):\n",
            "        count = 0\n",
            "        loss = 0\n",
            "\n",
            "        \n",
            "        # optimizer.zero_grad()\n",
            "        for inputs, targets in dataloader:\n",
            "            # inputs = inputs.permute(1,0,2)\n",
            "            # need another loop inside.\n",
            "            correct = 0\n",
            "            pred = 0\n",
            "            \n",
            "            # reset hidden here\n",
            "            loss = 0\n",
            "            optimizer.zero_grad()\n",
            "            # \n",
            "            for i in range(len(inputs)):\n",
            "                hidden= torch.zeros(54).to(device)\n",
            "                for j in range(len(inputs[i])):\n",
            "                    outputs = model(inputs[i][j].to(device), hidden)\n",
            "                    out, hidden = outputs[0:19], outputs[19::]\n",
            "                    # print(out.shape)\n",
            "                    # print(hidden.shape)\n",
            "                    # print(targets[i][j].shape)\n",
            "\n",
            "                    # print((torch.argmax(out) == targets[i][j]).item())\n",
            "                    correct += (torch.argmax(out) == targets[i][j]).item()\n",
            "                    pred += 1\n",
            "                    loss += criterion(out.to(device), targets[i][j].to(device)) / len(inputs[i]) / len(inputs)\n",
            "                # print(loss)\n",
            "            batch_accuracy = correct/pred\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "            count +=1\n",
            "            print(f\"Batch number: {count}\")\n",
            "            print(f\"Batch {count} accuracy: {batch_accuracy}\")\n",
            "        \n",
            "        # loss /= len(inputs)\n",
            "        # loss.backward()\n",
            "        # optimizer.step()\n",
            "\n",
            "        #     outputs = model(inputs.to(device), hidden)\n",
            "        #     out, hidden = outputs[0], outputs[1]\n",
            "        #     loss += criterion(out.to(device), targets.to(device))\n",
            "        # loss /= len(dataloader)\n",
            "        # loss.backward()\n",
            "        # optimizer.step()\n",
            "\n",
            "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Last Batch Accuracy: {batch_accuracy*100:.2f}%\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 36,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Batch number: 1\n",
                  "Batch 1 accuracy: 0.04041280864197531\n",
                  "Batch number: 2\n",
                  "Batch 2 accuracy: 0.04292052469135803\n",
                  "Batch number: 3\n",
                  "Batch 3 accuracy: 0.3392168209876543\n",
                  "Batch number: 4\n",
                  "Batch 4 accuracy: 0.3420138888888889\n",
                  "Batch number: 5\n",
                  "Batch 5 accuracy: 0.33130787037037035\n",
                  "Batch number: 6\n",
                  "Batch 6 accuracy: 0.31452546296296297\n",
                  "Batch number: 7\n",
                  "Batch 7 accuracy: 0.2934027777777778\n",
                  "Batch number: 8\n",
                  "Batch 8 accuracy: 0.28877314814814814\n",
                  "Batch number: 9\n",
                  "Batch 9 accuracy: 0.2916666666666667\n",
                  "Batch number: 10\n",
                  "Batch 10 accuracy: 0.29996141975308643\n",
                  "Batch number: 11\n",
                  "Batch 11 accuracy: 0.3187692901234568\n",
                  "Batch number: 12\n",
                  "Batch 12 accuracy: 0.31201774691358025\n",
                  "Batch number: 13\n",
                  "Batch 13 accuracy: 0.3056520061728395\n",
                  "Batch number: 14\n",
                  "Batch 14 accuracy: 0.33892746913580246\n",
                  "Batch number: 15\n",
                  "Batch 15 accuracy: 0.3351658950617284\n",
                  "Batch number: 16\n",
                  "Batch 16 accuracy: 0.33989197530864196\n",
                  "Batch number: 17\n",
                  "Batch 17 accuracy: 0.33805941358024694\n",
                  "Batch number: 18\n",
                  "Batch 18 accuracy: 0.3589891975308642\n",
                  "Batch number: 19\n",
                  "Batch 19 accuracy: 0.35281635802469136\n",
                  "Batch number: 20\n",
                  "Batch 20 accuracy: 0.3668016975308642\n",
                  "Batch number: 21\n",
                  "Batch 21 accuracy: 0.37355324074074076\n",
                  "Batch number: 22\n",
                  "Batch 22 accuracy: 0.3664158950617284\n",
                  "Batch number: 23\n",
                  "Batch 23 accuracy: 0.3681520061728395\n",
                  "Batch number: 24\n",
                  "Batch 24 accuracy: 0.37094907407407407\n",
                  "Batch number: 25\n",
                  "Batch 25 accuracy: 0.3759645061728395\n",
                  "Batch number: 26\n",
                  "Batch 26 accuracy: 0.37017746913580246\n",
                  "Batch number: 27\n",
                  "Batch 27 accuracy: 0.3792438271604938\n",
                  "Batch number: 28\n",
                  "Batch 28 accuracy: 0.3820408950617284\n",
                  "Batch number: 29\n",
                  "Batch 29 accuracy: 0.3864776234567901\n",
                  "Batch number: 30\n",
                  "Batch 30 accuracy: 0.4010416666666667\n",
                  "Batch number: 31\n",
                  "Batch 31 accuracy: 0.3994020061728395\n",
                  "Batch number: 32\n",
                  "Batch 32 accuracy: 0.39621913580246915\n",
                  "Batch number: 33\n",
                  "Batch 33 accuracy: 0.39737654320987653\n",
                  "Batch number: 34\n",
                  "Batch 34 accuracy: 0.4192708333333333\n",
                  "Batch number: 35\n",
                  "Batch 35 accuracy: 0.41280864197530864\n",
                  "Batch number: 36\n",
                  "Batch 36 accuracy: 0.42563657407407407\n",
                  "Batch number: 37\n",
                  "Batch 37 accuracy: 0.4224537037037037\n",
                  "Batch number: 38\n",
                  "Batch 38 accuracy: 0.4203317901234568\n",
                  "Batch number: 39\n",
                  "Batch 39 accuracy: 0.4294945987654321\n",
                  "Batch number: 40\n",
                  "Batch 40 accuracy: 0.41984953703703703\n",
                  "Batch number: 41\n",
                  "Batch 41 accuracy: 0.42023533950617287\n",
                  "Batch number: 42\n",
                  "Batch 42 accuracy: 0.42052469135802467\n",
                  "Batch number: 43\n",
                  "Batch 43 accuracy: 0.4392361111111111\n",
                  "Batch number: 44\n",
                  "Batch 44 accuracy: 0.4330632716049383\n",
                  "Batch number: 45\n",
                  "Batch 45 accuracy: 0.43171296296296297\n",
                  "Batch number: 46\n",
                  "Batch 46 accuracy: 0.4363425925925926\n",
                  "Batch number: 47\n",
                  "Batch 47 accuracy: 0.4380787037037037\n",
                  "Batch number: 48\n",
                  "Batch 48 accuracy: 0.4426118827160494\n",
                  "Batch number: 49\n",
                  "Batch 49 accuracy: 0.4449266975308642\n",
                  "Batch number: 50\n",
                  "Batch 50 accuracy: 0.4428047839506173\n",
                  "Batch number: 51\n",
                  "Batch 51 accuracy: 0.4444444444444444\n",
                  "Batch number: 52\n",
                  "Batch 52 accuracy: 0.45013503086419754\n",
                  "Batch number: 53\n",
                  "Batch 53 accuracy: 0.4347993827160494\n",
                  "Batch number: 54\n",
                  "Batch 54 accuracy: 0.4538966049382716\n",
                  "Batch number: 55\n",
                  "Batch 55 accuracy: 0.44309413580246915\n",
                  "Batch number: 56\n",
                  "Batch 56 accuracy: 0.44319058641975306\n",
                  "Batch number: 57\n",
                  "Batch 57 accuracy: 0.44753086419753085\n",
                  "Batch number: 58\n",
                  "Batch 58 accuracy: 0.4627700617283951\n",
                  "Batch number: 59\n",
                  "Batch 59 accuracy: 0.4565972222222222\n",
                  "Batch number: 60\n",
                  "Batch 60 accuracy: 0.4609375\n",
                  "Batch number: 61\n",
                  "Batch 61 accuracy: 0.45601851851851855\n",
                  "Batch number: 62\n",
                  "Batch 62 accuracy: 0.4574652777777778\n",
                  "Batch number: 63\n",
                  "Batch 63 accuracy: 0.45090663580246915\n",
                  "Batch number: 64\n",
                  "Batch 64 accuracy: 0.45987654320987653\n",
                  "Batch number: 65\n",
                  "Batch 65 accuracy: 0.4620949074074074\n",
                  "Batch number: 66\n",
                  "Batch 66 accuracy: 0.4722222222222222\n",
                  "Batch number: 67\n",
                  "Batch 67 accuracy: 0.47530864197530864\n",
                  "Batch number: 68\n",
                  "Batch 68 accuracy: 0.4607445987654321\n",
                  "Batch number: 69\n",
                  "Batch 69 accuracy: 0.4619020061728395\n",
                  "Batch number: 70\n",
                  "Batch 70 accuracy: 0.45360725308641975\n",
                  "Batch number: 71\n",
                  "Batch 71 accuracy: 0.46296296296296297\n",
                  "Batch number: 72\n",
                  "Batch 72 accuracy: 0.4645061728395062\n",
                  "Batch number: 73\n",
                  "Batch 73 accuracy: 0.4724151234567901\n",
                  "Batch number: 74\n",
                  "Batch 74 accuracy: 0.4732831790123457\n",
                  "Batch number: 75\n",
                  "Batch 75 accuracy: 0.47038966049382713\n",
                  "Batch number: 76\n",
                  "Batch 76 accuracy: 0.47280092592592593\n",
                  "Batch number: 77\n",
                  "Batch 77 accuracy: 0.46392746913580246\n",
                  "Batch number: 78\n",
                  "Batch 78 accuracy: 0.4756944444444444\n",
                  "Batch number: 79\n",
                  "Batch 79 accuracy: 0.4853395061728395\n",
                  "Epoch 1/1, Loss: 1.5803, Last Batch Accuracy: 48.53%\n"
               ]
            }
         ],
         "source": [
            "# Train the model\n",
            "model = RNN().to(device)\n",
            "train(model, dataloader, num_epochs = 1, learning_rate = 0.001, device = device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 20,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Save the model use new names each time tyvm\n",
            "torch.save(model.state_dict(), 'weights/model_1.pth')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "<All keys matched successfully>"
                  ]
               },
               "execution_count": 12,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Load the model\n",
            "model = RNN().to(device)\n",
            "model.load_state_dict(torch.load('weights/model_0.pth', map_location=torch.device('cpu')))\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "C:\\Users\\joen\\AppData\\Local\\Temp\\ipykernel_5176\\414264106.py:1: DtypeWarning: Columns (69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
                  "  test_dataset = pd.read_csv(\"dataset/training99_sort.csv\")\n"
               ]
            }
         ],
         "source": [
            "test_dataset = pd.read_csv(\"dataset/training99_sort.csv\")\n",
            "test_dataset.fillna(\"0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5,#\", inplace=True)\n",
            "test_rubiksdataset = RubiksDataset(test_dataset)\n",
            "test_dataloader = DataLoader(test_rubiksdataset, batch_size = 128)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 70,
         "metadata": {},
         "outputs": [],
         "source": [
            "def test(model, test_loader, device):\n",
            "    model.eval()\n",
            "    # test_loss = 0\n",
            "    correct = 0\n",
            "    pred = 0\n",
            "    count = 0\n",
            "    # criterion = torch.nn.CrossEntropyLoss()\n",
            "    with torch.no_grad():\n",
            "        for data, target in test_loader:\n",
            "            for i in range(len(data)):\n",
            "                hidden= torch.zeros(54).to(device)\n",
            "                for j in range(len(data[i])):\n",
            "                    output = model(data[i][j].to(device), hidden)\n",
            "                    out, hidden = output[0:19], output[19::]\n",
            "                    # test_loss += criterion(out.to(device), target[i][j].to(device)) / len(data[i]) / len(data)\n",
            "                    # pred = out.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
            "                    # correct += pred.eq(target.view_as(pred)).sum().item()\n",
            "                    # if args.dry_run:\n",
            "                    #     break\n",
            "                    correct += (torch.argmax(out) == target[i][j]).item()\n",
            "                    pred += 1\n",
            "            batch_accuracy = correct/pred\n",
            "\n",
            "    # test_loss /= len(test_loader.dataset)\n",
            "\n",
            "    # print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
            "    #     test_loss, correct, len(test_loader.dataset),\n",
            "    #     100. * correct / len(test_loader.dataset)))\n",
            "    count +=1\n",
            "    print(f\"Batch {count} accuracy: {batch_accuracy}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 71,
         "metadata": {},
         "outputs": [
            {
               "ename": "IndexError",
               "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
               "output_type": "error",
               "traceback": [
                  "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
                  "Input \u001b[1;32mIn [71]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
                  "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(model, test_loader, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m                 out, hidden \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m19\u001b[39m], output[\u001b[38;5;241m19\u001b[39m::]\n\u001b[0;32m     15\u001b[0m                 \u001b[38;5;66;03m# test_loss += criterion(out.to(device), target[i][j].to(device)) / len(data[i]) / len(data)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m                 pred \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# get the index of the max log-probability\u001b[39;00m\n\u001b[0;32m     17\u001b[0m                 correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39meq(target\u001b[38;5;241m.\u001b[39mview_as(pred))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     18\u001b[0m                 \u001b[38;5;66;03m# if args.dry_run:\u001b[39;00m\n\u001b[0;32m     19\u001b[0m                 \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[0;32m     20\u001b[0m                 \u001b[38;5;66;03m# correct += (torch.argmax(out) == target[i][j]).item()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#     test_loss, correct, len(test_loader.dataset),\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#     100. * correct / len(test_loader.dataset)))\u001b[39;00m\n",
                  "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
               ]
            }
         ],
         "source": [
            "test(model, test_dataloader, device=device)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# shawns stuff"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# split data into features\n",
            "targets_numpy = train.label.values\n",
            "features_numpy = train.loc[:,train.columns != \"label\"].values/255 # normalization\n",
            "\n",
            "# train test split. Size of train data is 80% and size of test data is 20%. \n",
            "features_train, features_test, targets_train, targets_test = train_test_split(features_numpy,\n",
            "                                                                             targets_numpy,\n",
            "                                                                             test_size = 0.2,\n",
            "                                                                             random_state = 42) \n",
            "\n",
            "# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
            "featuresTrain = torch.from_numpy(features_train)\n",
            "targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n",
            "\n",
            "# create feature and targets tensor for test set.\n",
            "featuresTest = torch.from_numpy(features_test)\n",
            "targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n",
            "\n",
            "# batch_size, epoch and iteration\n",
            "batch_size = 100\n",
            "n_iters = 10000\n",
            "num_epochs = n_iters / (len(features_train) / batch_size)\n",
            "num_epochs = int(num_epochs)\n",
            "\n",
            "# Pytorch train and test sets\n",
            "train = TensorDataset(featuresTrain,targetsTrain)\n",
            "test = TensorDataset(featuresTest,targetsTest)\n",
            "\n",
            "# data loader\n",
            "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
            "test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n",
            "\n",
            "# visualize one of the images in data set\n",
            "plt.imshow(features_numpy[10].reshape(28,28))\n",
            "plt.axis(\"off\")\n",
            "plt.title(str(targets_numpy[10]))\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "ename": "NameError",
               "evalue": "name 'features_train' is not defined",
               "output_type": "error",
               "traceback": [
                  "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                  "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     30\u001b[0m n_iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8000\u001b[39m\n\u001b[1;32m---> 31\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m n_iters \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[43mfeatures_train\u001b[49m) \u001b[38;5;241m/\u001b[39m batch_size)\n\u001b[0;32m     32\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(num_epochs)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Pytorch train and test sets\u001b[39;00m\n",
                  "\u001b[1;31mNameError\u001b[0m: name 'features_train' is not defined"
               ]
            }
         ],
         "source": [
            "# Create RNN Model\n",
            "class RNNModel(nn.Module):\n",
            "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
            "        super(RNNModel, self).__init__()\n",
            "        \n",
            "        # Number of hidden dimensions\n",
            "        self.hidden_dim = hidden_dim\n",
            "        \n",
            "        # Number of hidden layers\n",
            "        self.layer_dim = layer_dim\n",
            "        \n",
            "        # RNN\n",
            "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
            "        \n",
            "        # Readout layer\n",
            "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
            "    \n",
            "    def forward(self, x):\n",
            "        \n",
            "        # Initialize hidden state with zeros\n",
            "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
            "            \n",
            "        # One time step\n",
            "        out, hn = self.rnn(x, h0)\n",
            "        out = self.fc(out[:, -1, :]) \n",
            "        return out\n",
            "\n",
            "# batch_size, epoch and iteration\n",
            "batch_size = 100\n",
            "n_iters = 8000\n",
            "num_epochs = n_iters / (len(features_train) / batch_size)\n",
            "num_epochs = int(num_epochs)\n",
            "\n",
            "# Pytorch train and test sets\n",
            "train = TensorDataset(featuresTrain,targetsTrain)\n",
            "test = TensorDataset(featuresTest,targetsTest)\n",
            "\n",
            "# data loader\n",
            "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
            "test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n",
            "    \n",
            "# Create RNN\n",
            "input_dim = 28    # input dimension\n",
            "hidden_dim = 100  # hidden layer dimension\n",
            "layer_dim = 1     # number of hidden layers\n",
            "output_dim = 10   # output dimension\n",
            "\n",
            "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
            "\n",
            "# Cross Entropy Loss \n",
            "error = nn.CrossEntropyLoss()\n",
            "\n",
            "# SGD Optimizer\n",
            "learning_rate = 0.05\n",
            "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Training model\n",
            "seq_dim = 28  \n",
            "loss_list = []\n",
            "iteration_list = []\n",
            "accuracy_list = []\n",
            "count = 0\n",
            "for epoch in range(num_epochs):\n",
            "    for i, (images, labels) in enumerate(train_loader):\n",
            "\n",
            "        train  = Variable(images.view(-1, seq_dim, input_dim))\n",
            "        labels = Variable(labels )\n",
            "            \n",
            "        # Clear gradients\n",
            "        optimizer.zero_grad()\n",
            "        \n",
            "        # Forward propagation\n",
            "        outputs = model(train)\n",
            "        \n",
            "        # Calculate softmax and ross entropy loss\n",
            "        loss = error(outputs, labels)\n",
            "        \n",
            "        # Calculating gradients\n",
            "        loss.backward()\n",
            "        \n",
            "        # Update parameters\n",
            "        optimizer.step()\n",
            "        \n",
            "        count += 1\n",
            "        \n",
            "        if count % 250 == 0:\n",
            "            # Calculate Accuracy         \n",
            "            correct = 0\n",
            "            total = 0\n",
            "            # Iterate through test dataset\n",
            "            for images, labels in test_loader:\n",
            "                images = Variable(images.view(-1, seq_dim, input_dim))\n",
            "                \n",
            "                # Forward propagation\n",
            "                outputs = model(images)\n",
            "                \n",
            "                # Get predictions from the maximum value\n",
            "                predicted = torch.max(outputs.data, 1)[1]\n",
            "                \n",
            "                # Total number of labels\n",
            "                total += labels.size(0)\n",
            "                \n",
            "                correct += (predicted == labels).sum()\n",
            "            \n",
            "            accuracy = 100 * correct / float(total)\n",
            "            \n",
            "            # store loss and iteration\n",
            "            loss_list.append(loss.data)\n",
            "            iteration_list.append(count)\n",
            "            accuracy_list.append(accuracy)\n",
            "            if count % 500 == 0:\n",
            "                # Print Loss\n",
            "                print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data[0], accuracy))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# visualization loss \n",
            "plt.plot(iteration_list,loss_list)\n",
            "plt.xlabel(\"Number of iteration\")\n",
            "plt.ylabel(\"Loss\")\n",
            "plt.title(\"RNN: Loss vs Number of iteration\")\n",
            "plt.show()\n",
            "\n",
            "# visualization accuracy \n",
            "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
            "plt.xlabel(\"Number of iteration\")\n",
            "plt.ylabel(\"Accuracy\")\n",
            "plt.title(\"RNN: Accuracy vs Number of iteration\")\n",
            "plt.savefig('graph.png')\n",
            "plt.show()"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.5"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}